# MachineLearningGroup3FinalProject
# Student Depression Prediction â€“ SEIS 763-03

**Project goal:** build and evaluate a machine-learning pipeline that predicts whether a student is experiencing depression, based on academic-, lifestyle-, and stress-related factors.  
The work fulfils the semester-long class-project requirements (data â‰¥ 3 000 rows, multiple ML models, dimensionality reduction, k-fold CV, ensemble, IEEE paper, 20-min presentation).

## ğŸš€ Quick start

```bash
git clone https://github.com/<org-or-user>/student-depression-ml.git
cd student-depression-ml
conda env create -f environment.yml      # RAPIDS + scikit-learn + imbalanced-learn
conda activate seis763
jupyter lab
---

Open notebook.ipynb and run all cells (GPU recommended, but CPU fallback worksâ€”just slower).

ğŸ“‚ Repository layout
student-depression-ml/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ student_depression_dataset.csv     # original dataset (27â€‰901 rows)
â”‚
â”œâ”€â”€ notebook.ipynb            # full EDA + preprocessing + models + CV
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_knn.pkl          # tuned KNN (LDA-reduced) pipeline
â”‚   â””â”€â”€ best_svm.pkl          # tuned SVM pipeline
â”‚
â”œâ”€â”€ utils/                    # helper modules (will grow)
â”‚   â”œâ”€â”€ model_svm.py
â”‚   â””â”€â”€ model_knn.py
â”‚
â”œâ”€â”€ presentation/             # â† PowerPoint lives here
â”‚   â””â”€â”€ draft-deck.pptx
â”‚
â”œâ”€â”€ docs/                     # PDFs for grading
â”‚   â”œâ”€â”€ project_proposal.pdf
â”‚   â”œâ”€â”€ project_guide.pdf
â”‚   â””â”€â”€ notebook.pdf          # static export
â”‚
â”œâ”€â”€ environment.yml           # conda spec (CPU) + rapids.yaml (GPU optional)
â””â”€â”€ README.md

ğŸ§  Methodology (high level)
Cleaning & EDA â€“ handle outliers, standardise numeric features, one-hot encode categoricals.

Class balancing â€“ ADASYN over-sampling + random under-sampling (original split 58 / 42).

Dimensionality reduction â€“ PCA, Kernel PCA, and LDA (best for KNN).

Models tested

David â†’ K-Nearest Neighbours, Support Vector Machine

Karryn â†’ Logistic Regression, NaÃ¯ve Bayes

Cristian â†’ Random Forest, Decision Tree

Hyper-parameter tuning â€“ 10-fold Stratified CV, GridSearchCV (dask-ml when on GPU).

Evaluation â€“ F1, precision, recall, confusion matrices, ROC-AUC on held-out 10 % test set.

Ensemble (TBD) â€“ soft voting of each teammateâ€™s top model.

Current best single model: linear-kernel SVM (C = 0.1) â€“ F1 = 0.871 on the test set (10-fold CV: 0.871 Â± 0.004).
KNN (k = 11, LDA-1D) is ~1 pt behind.

ğŸ‘¥ Team

Name	Role & focus	GitHub	Contact
David J. Braun	Pre-processing Â· KNN & SVM owner Â· GPU optimisation	@DavidBraun777	davidjbraun777@gmail.com
Karryn J. Leake	Logistic Regression Â· NaÃ¯ve Bayes Â· Report editing	@kleake	leak3729@stthomas.edu
Cristian A. Zendejas	Random Forest Â· Decision Tree Â· Ensemble integration	@czendejas	zend7089@stthomas.edu
â° Project roadmap & status

Milestone	Target date	Status
Data cleaning & EDA	19 Apr	âœ…
Dimensionality-reduction prototype	20 Apr	âœ…
Hyper-param tuning (all six models)	27 Apr	â¬œ in progress
Model freeze & k-fold CV	02 May	â¬œ
Ensemble voting classifier	03 May	â¬œ
Slide deck draft (20 min)	04 May	â¬œ
Presentation dry-run	04 May	â¬œ
Final IEEE paper (6-8 pp)	13 May	â¬œ
Zip submission (report + code + data)	14 May	â¬œ
ğŸ“ To-do list (open issues)
 Finish grid-searches for Logistic Regression, NaÃ¯ve Bayes, Random Forest, Decision Tree

 Build ensemble â€“ soft voting vs. stacking; measure lift over SVM

 Add SHAP notebook for interpretability

 Polish PowerPoint deck â€“ high-res plots, flow diagram, rehearse script

 Write IEEE paper â€“ insert final metrics & figures

 Finalise environment.yml with exact versions

 Create GitHub release + Zenodo DOI

Track progress via GitHub Projects.

ğŸ¤ Contributing
Teammates: branch naming topic/<short-desc>, open a PR, request one review.
External PRs welcome after the course deadline.

ğŸ“œ License
Code: MIT. 
Dataset: see /data/LICENSE (original Kaggle terms).


